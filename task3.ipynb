{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction\n",
    "\n",
    "In this task you will explore linear and non-linear dimension reduction techniques, apply them to a dataset containing 28 × 28 pixel grayscale images – which, you may be surprised to hear, is not the usual MNIST.\n",
    "\n",
    "1. Load the Fashion-MNIST image dataset from link [1] but keep only the items in classes 5 (“sandal”), 7 (“sneaker”), and 9 (“angle boot”). You can use either all of the data from these classes or a smaller subset such as those in the 10000 item test set if you have trouble with efficiency. __Hint__: The data can be loaded using the exact same scripts that work for the standard MNIST data.\n",
    "\n",
    "\n",
    "2. Implement PCA. You are allowed to use ready-made functions for the eigenvalue decomposition (or singular value decomposition) but you shouldn’t use functions such as prcomp in R. __Hint__: Remember to center and normalize the data.\n",
    "\n",
    "\n",
    "3. Plot the data in terms of the first two principal components. Try to use colors, or even better use the images instead of the dots, to show the different classes so that you can tell whether the classes are separated or not.\n",
    "\n",
    "\n",
    "4. Now find an implementation of t-distributed stochastic neighbor embedding (t-SNE). Unlike above with PCA, you should use a ready-made implementation. Apply t-SNE to obtain a two-dimensional representation of the data and produce similar plots as you did with PCA. Compare the results. __Hint__: In R, try the package tsne. And python, try sklearn.manifold.TSNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist_reader import load_mnist\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (10000, 784) \n",
      "\n",
      "6000 6000 6000 18000\n",
      "1000 1000 1000 3000 \n",
      "\n",
      "(18000, 784) (3000, 784) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = ['t_shirt_top', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle_boots']\n",
    "\n",
    "X_train, y_train = load_mnist(\"data/fashion_mnist/\", kind=\"train\")\n",
    "X_test, y_test = load_mnist(\"data/fashion_mnist/\", kind=\"t10k\")\n",
    "print(X_train.shape, X_test.shape, \"\\n\")\n",
    "\n",
    "train_mask0 = (y_train == labels.index(\"sandal\"))\n",
    "train_mask1 = (y_train == labels.index(\"sneaker\"))\n",
    "train_mask2 = (y_train == labels.index(\"ankle_boots\"))\n",
    "train_mask = train_mask0 | train_mask1 | train_mask2\n",
    "print(train_mask0.sum(), train_mask1.sum(), train_mask2.sum(), train_mask.sum())\n",
    "test_mask0 = (y_test == labels.index(\"sandal\"))\n",
    "test_mask1 = (y_test == labels.index(\"sneaker\"))\n",
    "test_mask2 = (y_test == labels.index(\"ankle_boots\"))\n",
    "test_mask = test_mask0 | test_mask1 | test_mask2\n",
    "print(test_mask0.sum(), test_mask1.sum(), test_mask2.sum(), test_mask.sum(), \"\\n\")\n",
    "\n",
    "X_train, y_train = X_train[train_mask], y_train[train_mask]\n",
    "X_test, y_test = X_test[test_mask], y_test[test_mask]\n",
    "print(X_train.shape, X_test.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center data\n",
    "X_test_means = X_test.mean(axis=0)\n",
    "X_test = X_test - X_test_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U: (3000, 3000), s: (784,), Vh: (784, 784), S_: (3000, 784)\n"
     ]
    }
   ],
   "source": [
    "# use singular value decomposition to factorize matrix X_train into \n",
    "# U, s, Vh such that X_train == U @ s @ Vh\n",
    "U, s, Vh = linalg.svd(X_test)\n",
    "S_ = np.zeros(X_test.shape)\n",
    "for i in range(min(*X_test.shape)):\n",
    "    S_[i, i] = s[i]\n",
    "\n",
    "print(f\"U: {U.shape}, s: {s.shape}, Vh: {Vh.shape}, S_: {S_.shape}\")\n",
    "\n",
    "# check that reverse constructing original matrix from U, S_ and Vh produces\n",
    "tmp_a = np.dot(U, np.dot(S_, Vh))\n",
    "assert np.allclose(X_test, tmp_a), \"something went wrong in decomposing matrix\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select n principal components of X_test\n",
    "n_components = 5\n",
    "\n",
    "pca_mtx = U[:, :n_components]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 5)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_mtx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit ('venv': virtualenv)",
   "language": "python",
   "name": "python37264bitvenvvirtualenv981cb8530cb64c9f9ae58ce476aef944"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
