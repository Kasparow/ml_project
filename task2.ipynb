{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load 20news data with python\n",
    "\n",
    "The data is in form (group, word, frequency). This structure follows the COO sparse matrix format.\n",
    "\n",
    "The last column (frequency) is not used in this project.\n",
    "\n",
    "Finally print details to make sure loading data stats from here results the same values as provided by R example demo(0).\n",
    "\n",
    "__R Demo(0) stats__:\n",
    "- Number of groups: 20\n",
    "- Number of words: 53975\n",
    "- Number of documents: 11269\n",
    "- Number of word-doc pairs: 1467345\n",
    "- Density: 0.002412427"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix, csr_matrix, vstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1', '1')\n"
     ]
    }
   ],
   "source": [
    "def load_20news_data(path_pfix=\"data/20news/\"):\n",
    "    X = []\n",
    "    with open(f\"{path_pfix}train.data\", \"r\") as f:\n",
    "        for i, line in enumerate(f.readlines()):\n",
    "            line = line.split()\n",
    "            assert len(line) == 3, f\"line.split() returned incorrect number of elements: {len(line)} != 3\"\n",
    "            X.append(tuple(line[:2]))\n",
    "    \n",
    "    f = open(f\"{path_pfix}train.label\", \"r\")\n",
    "    y = [label.strip() for label in f.readlines()]\n",
    "    f.close()\n",
    "    \n",
    "    f = open(f\"{path_pfix}vocabulary.txt\", \"r\")\n",
    "    vocab = [token.strip() for token in f.readlines()]\n",
    "    f.close()\n",
    "    print(X[0])\n",
    "    return np.array(X, dtype=int), np.array(y, dtype=int), vocab\n",
    "\n",
    "\n",
    "X, y, vocab = load_20news_data()\n",
    "prev_docid = None\n",
    "docid_count = 0\n",
    "for x in X:\n",
    "    if x[0] != prev_docid:\n",
    "        docid_count += 1\n",
    "        prev_docid = x[0]\n",
    "\n",
    "assert docid_count == len(y), \\\n",
    "    \"Number of doc_ids and labels should be equal, got {docid_count} doc_ids and {len(y)} labels.\"\n",
    "\n",
    "groups = [\n",
    "    'alt.atheism', \n",
    "    'comp.graphics',    \n",
    "    'comp.os.ms-windows.misc', \n",
    "    'comp.sys.ibm.pc.hardware', \n",
    "    'comp.sys.mac.hardware', \n",
    "    'comp.windows.x', \n",
    "    'misc.forsale', \n",
    "    'rec.autos', \n",
    "    'rec.motorcycles', \n",
    "    'rec.sport.baseball', \n",
    "    'rec.sport.hockey', \n",
    "    'sci.crypt', \n",
    "    'sci.electronics', \n",
    "    'sci.med', \n",
    "    'sci.space', \n",
    "    'soc.religion.christian', \n",
    "    'talk.politics.guns',\n",
    "    'talk.politics.mideast', \n",
    "    'talk.politics.misc', \n",
    "    'talk.religion.misc'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• Number of groups: 20\n",
      "• Number of words: 53975\n",
      "• Number of documents: 11269\n",
      "• Number of word-doc-pairs: 1467345\n",
      "• Density: 0.002412427\n"
     ]
    }
   ],
   "source": [
    "print(f\"• Number of groups: {len(groups)}\")\n",
    "print(f\"• Number of words: {len(vocab)}\")\n",
    "print(f\"• Number of documents: {docid_count}\")\n",
    "print(f\"• Number of word-doc-pairs: {len(X)}\")\n",
    "print(f\"• Density: {round(len(X)/(docid_count*len(vocab)), 9)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ynp = np.array(y, dtype=int)\n",
    "group_index_ranges = []  # stores 2-tuples like (start_index, end_index) for each of the 20 groups\n",
    "train_index_ranges = []\n",
    "test_index_ranges = []\n",
    "for i in range(1, 21):\n",
    "    mask = (ynp == i)\n",
    "    idx_range = np.where(mask)[0]\n",
    "    split_idx = int((idx_range[-1]-idx_range[0])*0.9+idx_range[0])\n",
    "    group_index_ranges.append((idx_range[0], idx_range[-1]))\n",
    "    train_index_ranges.append((idx_range[0], split_idx))\n",
    "    test_index_ranges.append((split_idx, idx_range[-1]+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 431),\n",
       " (480, 1002),\n",
       " (1061, 1574),\n",
       " (1633, 2160),\n",
       " (2220, 2736),\n",
       " (2795, 3326),\n",
       " (3387, 3909),\n",
       " (3969, 4500),\n",
       " (4561, 5096),\n",
       " (5157, 5690),\n",
       " (5751, 6288),\n",
       " (6349, 6882),\n",
       " (6943, 7474),\n",
       " (7534, 8067),\n",
       " (8128, 8660),\n",
       " (8721, 9259),\n",
       " (9320, 9809),\n",
       " (9865, 10371),\n",
       " (10429, 10845),\n",
       " (10893, 11230)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_index_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(431, 480),\n",
       " (1002, 1061),\n",
       " (1574, 1633),\n",
       " (2160, 2220),\n",
       " (2736, 2795),\n",
       " (3326, 3387),\n",
       " (3909, 3969),\n",
       " (4500, 4561),\n",
       " (5096, 5157),\n",
       " (5690, 5751),\n",
       " (6288, 6349),\n",
       " (6882, 6943),\n",
       " (7474, 7534),\n",
       " (8067, 8128),\n",
       " (8660, 8721),\n",
       " (9259, 9320),\n",
       " (9809, 9865),\n",
       " (10371, 10429),\n",
       " (10845, 10893),\n",
       " (11230, 11269)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_index_ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create conditional probability tables\n",
    "\n",
    "\"Estimate the probability that a document from the given group contains the word word.\"\n",
    "\n",
    "Here we don't care about the word frequencies in documents, just the binary occurrence. Basically you need to count the number of documents in each group that has each word.\n",
    "\n",
    "`p(w|g) = #(docs_having_word and docs_in_group) / #docs_in_group`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.coo.coo_matrix"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = None  # groups\n",
    "col = None  # vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y-labels are in array thats length is the number of documents in the dataset. X on the other hand is currently in (implicit) sparse matrix format where first index indicates the document and second index the word. Construct a proper sparse matrix from X and concatenate Y to it.\n",
    "\n",
    "The dataset was intended to be used with R and for that reason the indexing starts with 1 instead of 0. Remove the first row and first column from resulting sparse matrix\n",
    "\n",
    "After the data in X is transformed, the documents can be directly aggregated based on group indexes given in y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11270, 53976)\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "X_csr = csr_matrix((np.ones(X.shape[0]), (X[:, 0], X[:, 1])))\n",
    "print(X_csr.shape)\n",
    "print(X_csr[:, 0].sum() == 0)\n",
    "print(X_csr[0, :].sum() == 0)\n",
    "X_csr = X_csr[:, range(1, X_csr.shape[1])]  # remove first column as its all-zeroes\n",
    "X_csr = X_csr[range(1, X_csr.shape[0]), :]  # remove first row as its all-zeroes\n",
    "assert X_csr.sum() == X.shape[0], f\"# of elements in sparse csr matrix does not match with the original\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11269, 53975)\n",
      "1467345.0\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(X_csr.shape)\n",
    "print(X_csr.sum())\n",
    "print(X_csr[:, 0].sum() == 0)\n",
    "print(X_csr[0, :].sum() == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431\n",
      "[[ 3. 33. 74.  4. 35. 30.  4.  1. 15. 48.]]\n",
      "[[7.35213028e-05 6.24931074e-04 1.37852443e-03 9.19016285e-05\n",
      "  6.61691725e-04 5.69790097e-04 9.19016285e-05 3.67606514e-05\n",
      "  2.94085211e-04 9.00635959e-04]]\n",
      "2\n",
      "\n",
      "522\n",
      "[[10. 34.  0.  7.  9. 39.  7.  5. 51.  3.]]\n",
      "[[2.01845973e-04 6.42237187e-04 1.83496339e-05 1.46797071e-04\n",
      "  1.83496339e-04 7.33985357e-04 1.46797071e-04 1.10097804e-04\n",
      "  9.54180964e-04 7.33985357e-05]]\n",
      "2\n",
      "\n",
      "513\n",
      "[[ 9. 39.  0. 10. 13. 32.  2.  2. 81.  0.]]\n",
      "[[1.83526648e-04 7.34106592e-04 1.83526648e-05 2.01879313e-04\n",
      "  2.56937307e-04 6.05637939e-04 5.50579944e-05 5.50579944e-05\n",
      "  1.50491851e-03 1.83526648e-05]]\n",
      "2\n",
      "\n",
      "527\n",
      "[[ 6. 18.  0.  0.  5. 41.  2.  1. 24.  0.]]\n",
      "[[1.28435654e-04 3.48611060e-04 1.83479505e-05 1.83479505e-05\n",
      "  1.10087703e-04 7.70613922e-04 5.50438516e-05 3.66959011e-05\n",
      "  4.58698763e-04 1.83479505e-05]]\n",
      "2\n",
      "\n",
      "516\n",
      "[[ 4. 19.  0.  1.  0. 35.  0.  0. 24.  0.]]\n",
      "[[9.17582720e-05 3.67033088e-04 1.83516544e-05 3.67033088e-05\n",
      "  1.83516544e-05 6.60659558e-04 1.83516544e-05 1.83516544e-05\n",
      "  4.58791360e-04 1.83516544e-05]]\n",
      "2\n",
      "\n",
      "531\n",
      "[[13. 46.  0. 18.  9. 28. 11.  3. 61.  0.]]\n",
      "[[2.56852457e-04 8.62290390e-04 1.83466040e-05 3.48585477e-04\n",
      "  1.83466040e-04 5.32051517e-04 2.20159249e-04 7.33864162e-05\n",
      "  1.13748945e-03 1.83466040e-05]]\n",
      "2\n",
      "\n",
      "522\n",
      "[[ 0. 18.  0.  2.  1. 23.  0.  0. 15.  3.]]\n",
      "[[1.83496339e-05 3.48643045e-04 1.83496339e-05 5.50489018e-05\n",
      "  3.66992678e-05 4.40391214e-04 1.83496339e-05 1.83496339e-05\n",
      "  2.93594143e-04 7.33985357e-05]]\n",
      "2\n",
      "\n",
      "531\n",
      "[[ 7. 36.  0.  0.  8. 66.  7.  3. 16.  0.]]\n",
      "[[1.46772832e-04 6.78824350e-04 1.83466040e-05 1.83466040e-05\n",
      "  1.65119436e-04 1.22922247e-03 1.46772832e-04 7.33864162e-05\n",
      "  3.11892269e-04 1.83466040e-05]]\n",
      "2\n",
      "\n",
      "535\n",
      "[[ 4. 32.  0.  2.  2. 65.  3.  1.  3.  0.]]\n",
      "[[9.17262888e-05 6.05393506e-04 1.83452578e-05 5.50357733e-05\n",
      "  5.50357733e-05 1.21078701e-03 7.33810310e-05 3.66905155e-05\n",
      "  7.33810310e-05 1.83452578e-05]]\n",
      "2\n",
      "\n",
      "533\n",
      "[[  1.  24.   0.   2.   1. 145.   1.   0.   2.   0.]]\n",
      "[[3.66918617e-05 4.58648272e-04 1.83459309e-05 5.50377926e-05\n",
      "  3.66918617e-05 2.67850591e-03 3.66918617e-05 1.83459309e-05\n",
      "  5.50377926e-05 1.83459309e-05]]\n",
      "2\n",
      "\n",
      "537\n",
      "[[  1.  35.   0.   0.   1. 120.   1.   6.   3.   0.]]\n",
      "[[3.66891694e-05 6.60405048e-04 1.83445847e-05 1.83445847e-05\n",
      "  3.66891694e-05 2.21969475e-03 3.66891694e-05 1.28412093e-04\n",
      "  7.33783387e-05 1.83445847e-05]]\n",
      "2\n",
      "\n",
      "533\n",
      "[[29. 55.  0.  7. 20. 53. 22.  4. 39.  0.]]\n",
      "[[5.50377926e-04 1.02737213e-03 1.83459309e-05 1.46767447e-04\n",
      "  3.85264548e-04 9.90680267e-04 4.21956410e-04 9.17296544e-05\n",
      "  7.33837235e-04 1.83459309e-05]]\n",
      "2\n",
      "\n",
      "531\n",
      "[[ 1. 18.  0.  1.  4. 21.  3.  1. 21.  0.]]\n",
      "[[3.66932081e-05 3.48585477e-04 1.83466040e-05 3.66932081e-05\n",
      "  9.17330202e-05 4.03625289e-04 7.33864162e-05 3.66932081e-05\n",
      "  4.03625289e-04 1.83466040e-05]]\n",
      "2\n",
      "\n",
      "533\n",
      "[[ 5. 24.  0.  8.  8. 45.  1.  4.  8.  0.]]\n",
      "[[1.10075585e-04 4.58648272e-04 1.83459309e-05 1.65113378e-04\n",
      "  1.65113378e-04 8.43912820e-04 3.66918617e-05 9.17296544e-05\n",
      "  1.65113378e-04 1.83459309e-05]]\n",
      "2\n",
      "\n",
      "532\n",
      "[[20. 53.  0. 15. 10. 61. 20.  7. 14.  0.]]\n",
      "[[3.85271616e-04 9.90698442e-04 1.83462675e-05 2.93540279e-04\n",
      "  2.01808942e-04 1.13746858e-03 3.85271616e-04 1.46770140e-04\n",
      "  2.75194012e-04 1.83462675e-05]]\n",
      "2\n",
      "\n",
      "538\n",
      "[[ 0. 61. 10.  5. 11. 65.  0.  0. 11. 37.]]\n",
      "[[1.83442482e-05 1.13734339e-03 2.01786730e-04 1.10065489e-04\n",
      "  2.20130978e-04 1.21072038e-03 1.83442482e-05 1.83442482e-05\n",
      "  2.20130978e-04 6.97081430e-04]]\n",
      "2\n",
      "\n",
      "489\n",
      "[[ 0. 24.  0.  6. 10. 82.  2.  0. 14.  0.]]\n",
      "[[1.83607521e-05 4.59018801e-04 1.83607521e-05 1.28525264e-04\n",
      "  2.01968273e-04 1.52394242e-03 5.50822562e-05 1.83607521e-05\n",
      "  2.75411281e-04 1.83607521e-05]]\n",
      "2\n",
      "\n",
      "506\n",
      "[[ 8. 67.  0.  7.  2. 97.  5. 19.  9.  1.]]\n",
      "[[1.65195206e-04 1.24814155e-03 1.83550229e-05 1.46840183e-04\n",
      "  5.50650686e-05 1.79879224e-03 1.10130137e-04 3.67100457e-04\n",
      "  1.83550229e-04 3.67100457e-05]]\n",
      "2\n",
      "\n",
      "416\n",
      "[[ 0. 25.  0. 13.  9. 85.  1. 11.  5.  0.]]\n",
      "[[1.83853946e-05 4.78020261e-04 1.83853946e-05 2.57395525e-04\n",
      "  1.83853946e-04 1.58114394e-03 3.67707893e-05 2.20624736e-04\n",
      "  1.10312368e-04 1.83853946e-05]]\n",
      "2\n",
      "\n",
      "337\n",
      "[[ 0. 28.  7.  2.  9. 30.  3.  1. 11.  4.]]\n",
      "[[1.84121373e-05 5.33951981e-04 1.47297098e-04 5.52364118e-05\n",
      "  1.84121373e-04 5.70776256e-04 7.36485491e-05 3.68242746e-05\n",
      "  2.20945647e-04 9.20606864e-05]]\n",
      "2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# so now to calculate p(w|g) we need to sum by row the subsets of X_csr defined by group indeces\n",
    "# and then divide each element by # docs in group (length of group index range)\n",
    "def empirical_posterior(X_csr, train_index_ranges):\n",
    "    prob_stack = []\n",
    "    for start, end in train_index_ranges:\n",
    "        vec = X_csr[start:end, :].sum(axis=0)\n",
    "        print(end-start)\n",
    "        num_docs = end-start\n",
    "        print(vec[0, :10])\n",
    "        vec = vec/num_docs\n",
    "        print(vec[0, :10])\n",
    "        print(vec.ndim)\n",
    "        print()\n",
    "        prob_stack.append(vec.A.reshape(vec.shape[1]))\n",
    "\n",
    "    prob_stack = np.array(prob_stack)\n",
    "    return prob_stack\n",
    "\n",
    "\n",
    "def laplace_smoothed_posterior(X_csr, train_index_ranges, alpha=1):\n",
    "    \"\"\"add-one smoothing when alpha=1\"\"\"\n",
    "    prob_stack = []\n",
    "    for start, end in train_index_ranges:\n",
    "        vec = X_csr[start:end, :].sum(axis=0)\n",
    "        print(end-start)\n",
    "        num_docs = end-start\n",
    "        print(vec[0, :10])\n",
    "        vec = (vec+alpha)/(num_docs+alpha*X_csr.shape[1])\n",
    "        print(vec[0, :10])\n",
    "        print(vec.ndim)\n",
    "        print()\n",
    "        prob_stack.append(vec.A.reshape(vec.shape[1]))\n",
    "\n",
    "    prob_stack = np.array(prob_stack)\n",
    "    return prob_stack\n",
    "\n",
    "\n",
    "prob_stack = laplace_smoothed_posterior(X_csr, train_index_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 53975)\n",
      "[7.35213028e-05 6.24931074e-04 1.37852443e-03 9.19016285e-05\n",
      " 6.61691725e-04 5.69790097e-04 9.19016285e-05 3.67606514e-05\n",
      " 2.94085211e-04 9.00635959e-04]\n"
     ]
    }
   ],
   "source": [
    "print(prob_stack.shape)\n",
    "print(prob_stack[0, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prob_stack is a matrix shape (groups, vocab) that keeps the probabilities for number of documents having a word divided by number of documents in a group. Both empirical posterior and with Laplace smoothing has now been applied. About Laplace/Additive smoothing: https://en.wikipedia.org/wiki/Additive_smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
